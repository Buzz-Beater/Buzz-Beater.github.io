<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v7.4.1 <https://qwtel.com/hydejack/>
-->




<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">


  
<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Inverse Reinforcement Learning | Baoxiong Jia @ Peking University</title>
<meta property="og:title" content="Inverse Reinforcement Learning" />
<meta name="author" content="Baoxiong Jia" />
<meta property="og:locale" content="en" />
<link rel="canonical" href="http://localhost:4000/2018/01/31/AlphaGo/" />
<meta property="og:url" content="http://localhost:4000/2018/01/31/AlphaGo/" />
<meta property="og:site_name" content="Baoxiong Jia @ Peking University" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-31T00:00:00+08:00" />
<script type="application/ld+json">
{"headline":"Inverse Reinforcement Learning","dateModified":"2018-01-31T00:00:00+08:00","datePublished":"2018-01-31T00:00:00+08:00","sameAs":null,"image":null,"author":{"@type":"Person","name":"Baoxiong Jia"},"description":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/01/31/AlphaGo/"},"name":null,"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/icons/icon.png"},"name":"Baoxiong Jia"},"@type":"BlogPosting","url":"http://localhost:4000/2018/01/31/AlphaGo/","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  

  
    <meta name="keywords" content="">
  


<meta name="mobile-web-app-capable" content="yes">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-title" content="Baoxiong Jia @ Peking University">
<meta name="apple-mobile-web-app-status-bar-style" content="black">

<meta name="application-name" content="Baoxiong Jia @ Peking University">
<meta name="msapplication-config" content="/assets/ieconfig.xml">


<meta name="theme-color" content="#4fb1ba">


<meta name="generator" content="Hydejack v7.4.1" />

<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Baoxiong Jia @ Peking University" />



<link rel="alternate" href="http://localhost:4000/2018/01/31/AlphaGo/" hreflang="en">

<link rel="shortcut icon" href="/assets/icons/favicon.ico">
<link rel="apple-touch-icon" href="/assets/icons/icon.png">

<link rel="manifest" href="/assets/manifest.json">


  <link rel="dns-prefetch" href="https://fonts.googleapis.com">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">




<link id="_katexJS"  rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.js">
<link id="_katexCSS" rel="dns-prefetch" href="/assets/bower_components/katex/dist/katex.min.css">

<script>
  function stdOnEnd(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function ieOnEnd(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}window.setRelStylesheet=function(n){function e(){this.rel="stylesheet"}var o=document.getElementById(n);o.addEventListener?o.addEventListener("load",e,!1):o.onload=e},window._loaded=!1,window.loadJSDeferred=function(n,e){function o(){window._loaded=!0;var o=document.createElement("script");o.src=n,e&&(("onload"in o?stdOnEnd:ieOnEnd)(o,e),o.onload||stdOnEnd(o,e));var t=document.scripts[0];t.parentNode.insertBefore(o,t)}window._loaded?o():window.addEventListener?window.addEventListener("load",o,!1):window.onload=o};
!function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this);
!function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this);

  window._noPushState = false;
  window._noDrawer = false;
</script>

<!--[if gt IE 8]><!---->


<script>
  WebFontConfig = {
    
    google: {
      families: ['Roboto+Slab:700','Noto+Sans:400,400i,700,700i']
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  };
  (function(d) {
    var wf = d.createElement('script'), s = d.scripts[0];
    wf.src = "/assets/bower_components/webfontloader/webfontloader.js";
    s.parentNode.insertBefore(wf, s);
  }(document));
</script>
<!--<![endif]-->

<noscript>
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i">
    <style>
      html { font-family: Noto Sans, Helvetica, Arial, sans-serif }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: Roboto Slab, Helvetica, Arial, sans-serif }
    </style>
  

  <link rel="stylesheet" href="/assets/icomoon/style.css">
</noscript>

<!--[if gt IE 8]><!---->



  <link rel="stylesheet" href="/assets/css/hydejack-7.4.1.css">



<style id="_pageStyle">

.content a:not(.btn){color:#4fb1ba;border-color:rgba(79,177,186,0.2)}.content a:not(.btn):hover{border-color:#4fb1ba}:focus{outline-color:#4fb1ba}.btn-primary{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:focus,.btn-primary.focus{box-shadow:0 0 0 3px rgba(79,177,186,0.5)}.btn-primary:hover,.btn-primary.hover{color:#fff;background-color:#409ba3;border-color:#409ba3}.btn-primary:disabled,.btn-primary.disabled{color:#fff;background-color:#4fb1ba;border-color:#4fb1ba}.btn-primary:active,.btn-primary.active{color:#fff;background-color:#409ba3;border-color:#409ba3}::selection{color:#fff;background:#4fb1ba}::-moz-selection{color:#fff;background:#4fb1ba}

</style>

<!--<![endif]-->




</head>

<body>
  <div class="navbar fixed-top">
  <div class="content">
    <div class="nav-btn-bar">
      <span class="sr-only">Jump to:</span>
      <a id="_menu" class="nav-btn no-hover" href="#_navigation">
        <span class="sr-only">Navigation</span>
        <span class="icon-menu"></span>
      </a>
    </div>
  </div>
</div>


<hy-push-state>
  <main
    id="_main"
    class="content fade-in layout-post"
    role="main"
    data-color="#4fb1ba"
    data-theme-color=""
    
      data-image="/assets/img/sidebar-bg.jpg
"
      data-overlay
    
    >
    


<article id="post-2018-01-31-AlphaGo" class="page post" role="article" style="text-align: justify">
  <header>
    <h1 class="post-title">
      
        Inverse Reinforcement Learning
      
    </h1>

    <p class="post-date heading">
      
      <time datetime="2018-01-31T00:00:00+08:00">31 Jan 2018</time>
      
      
      
      
      











      









on <span>Reinforcement learning</span>

    </p>

    



  <div class="hr pb0"></div>


  </header>

  
    <p>Inverse reinforcement learning (IRL) is the problem of extracting a reward function given observed, optimal behavior.</p>

<p>The motivation of forming up such a problem comes from two key observations</p>

<ul>
  <li>
    <p>(1) In examining animal and human behavior, the reward function is often unknown, especially in the case of multi-attribute reward functions (which is the common situation).</p>
  </li>
  <li>
    <p>(2) When building up a intelligent agent, designers may have only a very rough idea of what the reward function should be like for the agent to behave successfully in some certain domain.</p>
  </li>
</ul>

<p>As commonly used in other learning frameworks, one source of information can be obtained by imitating the behavior of some “expert” agents. However, it is believed that compared to learning the policies (actions <code class="MathJax_Preview">a_t</code><script type="math/tex">a_t</script>, current state <code class="MathJax_Preview">s_t</code><script type="math/tex">s_t</script> and next state <code class="MathJax_Preview">s_{t+1}</code><script type="math/tex">s_{t+1}</script>), the underlying reward function is a more robust definition of the task. Thus comes the idea of inverse reinforcement learning, that is, treats the observed “expert” actions as optimal behaviors to learn the underlying reward function in different tasks. Inverse optimal control or inverse optimal planning are also used to address this specific problem.</p>

<h2 id="formulation">Formulation</h2>

<h3 id="1-markov-decision-process-mdp">1. Markov Decision Process (MDP)</h3>
<p>A (finite) MDP is a tuple <code class="MathJax_Preview">(S,\ A,\ T,\ \gamma,\ R,\ D)</code><script type="math/tex">(S,\ A,\ T,\ \gamma,\ R,\ D)</script>, where</p>
<ul>
  <li>
    <p><code class="MathJax_Preview">S</code><script type="math/tex">S</script> is a finite set of <code class="MathJax_Preview">N</code><script type="math/tex">N</script> <strong>states</strong>.</p>
  </li>
  <li>
    <p><code class="MathJax_Preview">A = \{a_1, ..., a_k\}</code><script type="math/tex">A = \{a_1, ..., a_k\}</script> is a set of <code class="MathJax_Preview">k</code><script type="math/tex">k</script> <strong>actions</strong>.</p>
  </li>
  <li>
    <p><code class="MathJax_Preview">T(s'\| s, a) = T(S_{t+1} = s' \| S_t = s, A_t = a)</code><script type="math/tex">T(s'\| s, a) = T(S_{t+1} = s' \| S_t = s, A_t = a)</script> is a <strong>state transition probabilities</strong> upon taking action <code class="MathJax_Preview">a</code><script type="math/tex">a</script> in state <code class="MathJax_Preview">s</code><script type="math/tex">s</script>.</p>
  </li>
  <li>
    <p><code class="MathJax_Preview">R: S\to\mathbb{R}</code><script type="math/tex">R: S\to\mathbb{R}</script> is the <strong>reinforcement</strong> function or the <strong>reward function</strong>.</p>

    <p>Normally we can define <code class="MathJax_Preview">R = R(s,a) = \mathbb{E}[R_{t+1}\|S_t=s, A_t=a]</code><script type="math/tex">R = R(s,a) = \mathbb{E}[R_{t+1}\|S_t=s, A_t=a]</script> or <code class="MathJax_Preview">R = R(s) = \mathbb{E}_a[R(s, a)]</code><script type="math/tex">R = R(s) = \mathbb{E}_a[R(s, a)]</script></p>
  </li>
  <li>
    <p><code class="MathJax_Preview">\gamma \in [0, 1)</code><script type="math/tex">\gamma \in [0, 1)</script> which is the discount factor for taking future events into account.</p>
  </li>
  <li>
    <p><code class="MathJax_Preview">D</code><script type="math/tex">D</script> indicates the initial distribution for starting state $s_0$</p>
  </li>
</ul>

<h4 id="2-policy-value-function-and-q-function">2. Policy, Value Function and Q-function</h4>
<p>A <strong>policy</strong> <code class="MathJax_Preview">\pi: S \to A</code><script type="math/tex">\pi: S \to A</script> is a mapping from state space to action space, which is the basic guide of the decision making process. It is often defined as
<code class="MathJax_Preview">\pi(a\|s) = Pr(A_t = a \| S_t = s)</code><script type="math/tex">\pi(a\|s) = Pr(A_t = a \| S_t = s)</script>
With policy <code class="MathJax_Preview">\pi</code><script type="math/tex">\pi</script>, the <strong>value function</strong> <code class="MathJax_Preview">V^{\pi}(s)</code><script type="math/tex">V^{\pi}(s)</script> can be defined as
<code class="MathJax_Preview">\begin{aligned}
V^{\pi}(s_t) &amp; = \mathbb{E}_{(s_t, s_{t+1}, ...)}[G_t\ |\ S_t = s_t]\\
&amp; = \mathbb{E}_{(s_t, s_{t+1}, ...)}[R(s_t) + \gamma R(s_{t+1}) + \gamma^2R(s_{t+2}) + ... | \pi]
\end{aligned}</code><script type="math/tex">% <![CDATA[
\begin{aligned}
V^{\pi}(s_t) & = \mathbb{E}_{(s_t, s_{t+1}, ...)}[G_t\ |\ S_t = s_t]\\
& = \mathbb{E}_{(s_t, s_{t+1}, ...)}[R(s_t) + \gamma R(s_{t+1}) + \gamma^2R(s_{t+2}) + ... | \pi]
\end{aligned} %]]></script>
where <code class="MathJax_Preview">G_t = \sum_{k = 0}^{\infty} \gamma^kR(s_{t+k})</code><script type="math/tex">G_t = \sum_{k = 0}^{\infty} \gamma^kR(s_{t+k})</script> is the discounted reward from time <code class="MathJax_Preview">t</code><script type="math/tex">t</script>.
We can also define the <strong>Q-function</strong> as follows
<code class="MathJax_Preview">\begin{aligned}
Q^{\pi}(s_t, a_t) &amp; = \mathbb{E}_{s_{t+1} \sim T(s_{t+1} | s_t, a_t)}[G_t | S_t = s_t, A_t = a_t]\\
&amp; = R(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]
\end{aligned}</code><script type="math/tex">% <![CDATA[
\begin{aligned}
Q^{\pi}(s_t, a_t) & = \mathbb{E}_{s_{t+1} \sim T(s_{t+1} | s_t, a_t)}[G_t | S_t = s_t, A_t = a_t]\\
& = R(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]
\end{aligned} %]]></script>
While <strong>value function</strong> tries to evaluate the value of a state <code class="MathJax_Preview">s</code><script type="math/tex">s</script>, the <strong>Q-function</strong> tries to evaluate the value of certain action given a starting state <code class="MathJax_Preview">s</code><script type="math/tex">s</script> (use action <code class="MathJax_Preview">a</code><script type="math/tex">a</script> in this step and take action according to policy <code class="MathJax_Preview">\pi</code><script type="math/tex">\pi</script> in future steps). This two functions are related in the form shown below:
<code class="MathJax_Preview">V^{\pi}(s_t) = \sum_{a_t}\pi(a_t|s_t)Q^{\pi}(s_t, a_t)</code><script type="math/tex">V^{\pi}(s_t) = \sum_{a_t}\pi(a_t|s_t)Q^{\pi}(s_t, a_t)</script></p>

<p>The <strong>optimal value function</strong> and <strong>optimal Q-function</strong> can thus be defined as
<code class="MathJax_Preview">\begin{aligned}
    V^{*}(s)\ &amp; = sup_{\pi}\ V^{\pi}(s)\\
    Q^{*}(s, a)\ &amp; = sup_{\pi}\ Q^{\pi}(s,a)
\end{aligned}</code><script type="math/tex">% <![CDATA[
\begin{aligned}
    V^{*}(s)\ & = sup_{\pi}\ V^{\pi}(s)\\
    Q^{*}(s, a)\ & = sup_{\pi}\ Q^{\pi}(s,a)
\end{aligned} %]]></script>
In the standard reinforcement learning problem, the goal is to find the <strong>optimal policy</strong> $\pi^<em>$ such that $V^{\pi^{</em>}}(s)$ is maximized for all $s\in S$.</p>

<h4 id="3-bellman-equations-and-bellman-optimality">3. Bellman Equations and Bellman Optimality</h4>
<p>In fact, we can have more precise definitions for both the <strong>value function</strong> and the <strong>Q-function</strong>. First, in the <strong>value function</strong>, we can use $V^{\pi}(s_{t+1})$ to substitute the infinite series:
$$
\begin{aligned}
V^{\pi}(s_t) &amp; = \mathbb{E}<em>{(s_t, s</em>{t+1}, …)}[R(s_t) + \gamma R(s_{t+1}) + \gamma^2R(s_{t+2}) + … | \pi] \</p>

<p>&amp; = \sum_{a_t\in A} \pi(a_t|s_t)\left(R(s_t, a_t) + \gamma \sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)V^{\pi}(s_{t+1})\right) <br />
&amp; = R(s_t) + \gamma \sum_{a_t\in A}\sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)\pi(a_t|s_t)V^{\pi}(s_{t+1})<br />
&amp; = \mathbb{E}<em>{(s_t, s</em>{t+1}, …)} [R(s_t) + \gamma V^{\pi}(s_{t+1})]
\end{aligned}
<code class="MathJax_Preview">Similarly, for **Q-function** we have</code><script type="math/tex">Similarly, for **Q-function** we have</script>
\begin{aligned}
Q^{\pi}(s_t, a_t)\ &amp; = R(s_t, a_t) + \mathbb{E}<em>{s</em>{t+1}\sim T(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]<br />
&amp; = R(s_t, a_t) + \mathbb{E}<em>{s</em>{t+1}\sim T(s_{t+1}|s_t, a_t)}[R(s_{t+1}) + V^{\pi}(s_{t+2})]<br />
&amp; = R(s_t, a_t) + \gamma \sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)\sum_{a_{t+1}\in A}\pi(a_{t+1}| s_{t+1})Q^{\pi}(s_{t+1}, a_{t+1})<br />
&amp; = \mathbb{E}<em>{s</em>{t+1}\sim T(s_{t+1}|s_t, a_t)}[R(s_t, a_t) + \gamma Q^{\pi}(s_{t+1}, a_{t+1})]
\end{aligned}
$$</p>

<p>To find the optimal policy $\pi^*$, we can either maximize over <strong>value function</strong> or <strong>Q-function</strong>
<code class="MathJax_Preview">\begin{aligned}
V^*(s_t) &amp; = \max_{a}{R(s_t, a) + \gamma\sum_{s_{t+1}\in S}T(s_{t+1} | s_t, a)V^{*}(s_{t+1})}\\
&amp; = \max_{a} Q^*(s_t, a)
\end{aligned}</code><script type="math/tex">% <![CDATA[
\begin{aligned}
V^*(s_t) & = \max_{a}{R(s_t, a) + \gamma\sum_{s_{t+1}\in S}T(s_{t+1} | s_t, a)V^{*}(s_{t+1})}\\
& = \max_{a} Q^*(s_t, a)
\end{aligned} %]]></script>
The optimal policy can be acquired through
<code class="MathJax_Preview">\begin{aligned}
  \pi(a|s) = \begin{cases}
      1, &amp; \text{if}\ a={\rm arg}\max Q^*(s, a) \\
      0, &amp; \text{otherwise}
    \end{cases}
\end{aligned}</code><script type="math/tex">% <![CDATA[
\begin{aligned}
  \pi(a|s) = \begin{cases}
      1, & \text{if}\ a={\rm arg}\max Q^*(s, a) \\
      0, & \text{otherwise}
    \end{cases}
\end{aligned} %]]></script></p>

  
</article>


<hr class="dingbat related" />




  
    




  

  
    




<aside class="related mb4" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      


<li>
  <a href="/2018/01/31/IRL/" class="h4 flip-title">
    <span>From AlphaGo to AlphaGo Zero</span>
  </a>
  <time class="heading faded fine" datetime="2018-01-31T00:00:00+08:00">31 Jan 2018</time>
</li>

    
  </ul>
</aside>


  


    


    <footer role="contentinfo">
  <hr/>
  
  <p><small class="copyright">© 2018. All rights reserved.
</small></p>
  
  
  <hr class="sr-only"/>
</footer>

  </main>
  <hy-drawer>
  <header id="_sidebar" class="sidebar" role="banner">
    
    <div class="sidebar-bg sidebar-overlay" style="background-color:#4fb1ba;background-image:url(/assets/img/sidebar-bg.jpg
)"></div>

    <div class="sidebar-sticky">
      <div class="sidebar-about">
        <h2 class="h1"><a href="/">Baoxiong Jia</a></h2>
        
        
          <p class="">
            Buzz Beater

          </p>
        
      </div>

      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  
  
  
  
    
      <li>
        <a
          id="_navigation"
          href="/publications/"
          class="sidebar-nav-item"
          
          >
          Publication
        </a>
      </li>
    
  
    
      <li>
        <a
          
          href="/blog/"
          class="sidebar-nav-item"
          
          >
          Notes
        </a>
      </li>
    
  
</ul>

      </nav>

      

      <div class="sidebar-social">
        <span class="sr-only">Social:</span>
<ul>
  
    
    
  

  

  
    













<li>
  <a href="mailto:jerryjiabx@gmail.com" title="Email" class="no-mark-external">
    <span class="icon-mail"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
    













<li>
  <a href="https://github.com/Buzz-Beater" title="GitHub" class="no-mark-external">
    <span class="icon-github"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    













<li>
  <a href="https://facebook.com/baoxiongjia.buzzbeater" title="Facebook" class="no-mark-external">
    <span class="icon-facebook"></span>
    <span class="sr-only">Facebook</span>
  </a>
</li>

  
    













<li>
  <a href="https://www.linkedin.com/in/baoxiong-jia-2b6094122" title="LinkedIn" class="no-mark-external">
    <span class="icon-linkedin2"></span>
    <span class="sr-only">LinkedIn</span>
  </a>
</li>

  
</ul>

      </div>
    </div>
  </header>
</hy-drawer>

</hy-push-state>


  

  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack-7.4.1.js');</script>
  
  <!--<![endif]-->



  
<hr class="sr-only"/>
<h2 class="sr-only">Templates (for web app):</h2>

<template id="_animation-template">
  <div class="animation-main fixed-top">
    <div class="content">
      <div class="page"></div>
    </div>
  </div>
</template>

<template id="_loading-template">
  <div class="loading">
    <span class="sr-only">Loading…</span>
    <span class="icon-cog"></span>
  </div>
</template>

<template id="_error-template">
  <div class="page">
    <h1 class="page-title">Error</h1>
    
    
    <p class="lead">
      Sorry, an error occurred while loading <a class="this-link" href=""></a>.

    </p>
  </div>
</template>

<template id="_back-template">
  <a id="_back" class="back nav-btn no-hover">
    <span class="sr-only">Back</span>
    <span class="icon-arrow-left2"></span>
  </a>
</template>



</body>
</html>
