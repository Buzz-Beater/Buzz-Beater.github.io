---
layout: post
title: Inverse Reinforcement Learning
description: >
tags: [Reinforcement Learning]
---
Inverse reinforcement learning (IRL) is the problem of extracting a reward function given observed, optimal behavior.

The motivation of forming up such a problem comes from two key observations

- (1) In examining animal and human behavior, the reward function is often unknown, especially in the case of multi-attribute reward functions (which is the common situation).

- (2) When building up a intelligent agent, designers may have only a very rough idea of what the reward function should be like for the agent to behave successfully in some certain domain.

As commonly used in other learning frameworks, one source of information can be obtained by imitating the behavior of some "expert" agents. However, it is believed that compared to learning the policies (actions $$a_t$$, current state $$s_t$$ and next state $$s_{t+1}$$), the underlying reward function is a more robust definition of the task. Thus comes the idea of inverse reinforcement learning, that is, treats the observed "expert" actions as optimal behaviors to learn the underlying reward function in different tasks. Inverse optimal control or inverse optimal planning are also used to address this specific problem.

## Formulation

### 1. Markov Decision Process (MDP)
A (finite) MDP is a tuple $$(S,\ A,\ T,\ \gamma,\ R,\ D)$$, where
- $$S$$ is a finite set of $$N$$ **states**.

- $$A = \{a_1, ..., a_k\}$$ is a set of $$k$$ **actions**.

- $$ T(s'\| s, a) = T(S_{t+1} = s' \| S_t = s, A_t = a) $$ is a **state transition probabilities** upon taking action $$a$$ in state $$s$$.

- $$R: S\to\mathbb{R}$$ is the **reinforcement** function or the **reward function**.

  Normally we can define $$R = R(s,a) = \mathbb{E}[R_{t+1}\|S_t=s, A_t=a]$$ or $$R = R(s) = \mathbb{E}_a[R(s, a)]$$

- $$\gamma \in [0, 1)$$ which is the discount factor for taking future events into account.

- $$D$$ indicates the initial distribution for starting state $s_0$

#### 2. Policy, Value Function and Q-function
A **policy** $$\pi: S \to A$$ is a mapping from state space to action space, which is the basic guide of the decision making process. It is often defined as
$$
\pi(a\|s) = Pr(A_t = a \| S_t = s)
$$
With policy $$\pi$$, the **value function** $$V^{\pi}(s)$$ can be defined as
$$
\begin{aligned}
V^{\pi}(s_t) & = \mathbb{E}_{(s_t, s_{t+1}, ...)}[G_t\ |\ S_t = s_t]\\
& = \mathbb{E}_{(s_t, s_{t+1}, ...)}[R(s_t) + \gamma R(s_{t+1}) + \gamma^2R(s_{t+2}) + ... | \pi]
\end{aligned}
$$
where $$G_t = \sum_{k = 0}^{\infty} \gamma^kR(s_{t+k})$$ is the discounted reward from time $$t$$.
We can also define the **Q-function** as follows
$$
\begin{aligned}
Q^{\pi}(s_t, a_t) & = \mathbb{E}_{s_{t+1} \sim T(s_{t+1} | s_t, a_t)}[G_t | S_t = s_t, A_t = a_t]\\
& = R(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]
\end{aligned}
$$
While **value function** tries to evaluate the value of a state $$s$$, the **Q-function** tries to evaluate the value of certain action given a starting state $$s$$ (use action $$a$$ in this step and take action according to policy $$\pi$$ in future steps). This two functions are related in the form shown below:
$$
V^{\pi}(s_t) = \sum_{a_t}\pi(a_t|s_t)Q^{\pi}(s_t, a_t)
$$

The **optimal value function** and **optimal Q-function** can thus be defined as
$$
\begin{aligned}
    V^{*}(s)\ & = sup_{\pi}\ V^{\pi}(s)\\
    Q^{*}(s, a)\ & = sup_{\pi}\ Q^{\pi}(s,a)
\end{aligned}
$$
In the standard reinforcement learning problem, the goal is to find the **optimal policy** $\pi^*$ such that $V^{\pi^{*}}(s)$ is maximized for all $s\in S$.

#### 3. Bellman Equations and Bellman Optimality
In fact, we can have more precise definitions for both the **value function** and the **Q-function**. First, in the **value function**, we can use $V^{\pi}(s_{t+1})$ to substitute the infinite series:
$$
\begin{aligned}
V^{\pi}(s_t) & = \mathbb{E}_{(s_t, s_{t+1}, ...)}[R(s_t) + \gamma R(s_{t+1}) + \gamma^2R(s_{t+2}) + ... | \pi] \\

& = \sum_{a_t\in A} \pi(a_t|s_t)\left(R(s_t, a_t) + \gamma \sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)V^{\pi}(s_{t+1})\right) \\
& = R(s_t) + \gamma \sum_{a_t\in A}\sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)\pi(a_t|s_t)V^{\pi}(s_{t+1})\\
& = \mathbb{E}_{(s_t, s_{t+1}, ...)} [R(s_t) + \gamma V^{\pi}(s_{t+1})]
\end{aligned}
$$
Similarly, for **Q-function** we have
$$
\begin{aligned}
Q^{\pi}(s_t, a_t)\ & = R(s_t, a_t) + \mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[V^{\pi}(s_{t+1})]\\
& = R(s_t, a_t) + \mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[R(s_{t+1}) + V^{\pi}(s_{t+2})]\\
& = R(s_t, a_t) + \gamma \sum_{s_{t+1}\in S}T(s_{t+1}|s_t, a_t)\sum_{a_{t+1}\in A}\pi(a_{t+1}| s_{t+1})Q^{\pi}(s_{t+1}, a_{t+1})\\
& = \mathbb{E}_{s_{t+1}\sim T(s_{t+1}|s_t, a_t)}[R(s_t, a_t) + \gamma Q^{\pi}(s_{t+1}, a_{t+1})]
\end{aligned}
$$

To find the optimal policy $\pi^*$, we can either maximize over **value function** or **Q-function**
$$
\begin{aligned}
V^*(s_t) & = \max_{a}{R(s_t, a) + \gamma\sum_{s_{t+1}\in S}T(s_{t+1} | s_t, a)V^{*}(s_{t+1})}\\
& = \max_{a} Q^*(s_t, a)
\end{aligned}
$$
The optimal policy can be acquired through
$$
\begin{aligned}
  \pi(a|s) = \begin{cases}
      1, & \text{if}\ a={\rm arg}\max Q^*(s, a) \\
      0, & \text{otherwise}
    \end{cases}
\end{aligned}
$$
